{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab15aee",
   "metadata": {},
   "source": [
    "\n",
    "# Documentación y Perfilado del Dataset — NHAMCS ED 2022\n",
    "\n",
    "Este notebook **documenta** y **perfila** el dataset convertido desde SAS a CSV, y **genera automáticamente**:\n",
    "- `DATADICT.md` (diccionario de datos por columna)\n",
    "- `DATASET.md` (ficha técnica del dataset)\n",
    "- `PROVENANCE.md` (origen y trazabilidad)\n",
    "- `SHA256SUMS.txt` (hash de verificación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf1c57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE      : D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\n",
      "DATA_DIR  : D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\data\n",
      "OUT_DIR   : D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\n",
      "CSV_PATH  : D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\ed2022.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Base = carpeta del notebook (portabilidad)\n",
    "BASE = Path.cwd()\n",
    "\n",
    "# Carpetas de trabajo (crear si no existen)\n",
    "DATA_DIR = BASE / \"data\"\n",
    "OUT_DIR  = BASE / \"dataset_docs\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CSV objetivo de trabajo (ajustá el nombre si usás otro)\n",
    "CSV_PATH = DATA_DIR / \"ed2022_clean_min.csv\"\n",
    "\n",
    "# (Opcional) Backups por compatibilidad: si venís de una ruta vieja absoluta, la usamos una sola vez\n",
    "LEGACY_CSVS = [\n",
    "    Path(r\"D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\ed2022_clean_min.csv\"),\n",
    "    Path(r\"D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\ed2022.csv\"),\n",
    "]\n",
    "if not CSV_PATH.exists():\n",
    "    for p in LEGACY_CSVS:\n",
    "        if p.exists():\n",
    "            CSV_PATH = p\n",
    "            break\n",
    "\n",
    "print(\"BASE      :\", BASE)\n",
    "print(\"DATA_DIR  :\", DATA_DIR)\n",
    "print(\"OUT_DIR   :\", OUT_DIR)\n",
    "print(\"CSV_PATH  :\", CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86592f82",
   "metadata": {},
   "source": [
    "## 1 Carga y vista preliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b637f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VMONTH</th>\n",
       "      <th>VDAYR</th>\n",
       "      <th>ARRTIME</th>\n",
       "      <th>WAITTIME</th>\n",
       "      <th>LOV</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGER</th>\n",
       "      <th>AGEDAYS</th>\n",
       "      <th>RESIDNCE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>...</th>\n",
       "      <th>RX30V3C2</th>\n",
       "      <th>RX30V3C3</th>\n",
       "      <th>RX30V3C4</th>\n",
       "      <th>SETTYPE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>CSTRATM</th>\n",
       "      <th>CPSUM</th>\n",
       "      <th>PATWT</th>\n",
       "      <th>EDWT</th>\n",
       "      <th>BOARDED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>604</td>\n",
       "      <td>10.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>20122201.0</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>3665.56954</td>\n",
       "      <td>8.36413</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1053</td>\n",
       "      <td>40.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>20122201.0</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>3665.56954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1419</td>\n",
       "      <td>70.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>20122201.0</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>3665.56954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>20122201.0</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>3665.56954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2243</td>\n",
       "      <td>14.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>20122201.0</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>3665.56954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>903</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>20122201.0</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>3665.56954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1428</td>\n",
       "      <td>34.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>20122201.0</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>3665.56954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1830</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>20122201.0</td>\n",
       "      <td>100001.0</td>\n",
       "      <td>3665.56954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 913 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   VMONTH  VDAYR  ARRTIME  WAITTIME    LOV   AGE  AGER  AGEDAYS  RESIDNCE  \\\n",
       "0     9.0    2.0      604      10.0  228.0  23.0   2.0     -7.0       1.0   \n",
       "1     9.0    2.0     1053      40.0  319.0  15.0   2.0     -7.0       1.0   \n",
       "2     9.0    2.0     1419      70.0  551.0  19.0   2.0     -7.0       1.0   \n",
       "3     9.0    2.0     1825      -7.0   -9.0   0.0   1.0    298.0       1.0   \n",
       "4     9.0    2.0     2243      14.0  168.0  18.0   2.0     -7.0       1.0   \n",
       "5     9.0    3.0      903       2.0  113.0  39.0   3.0     -7.0       1.0   \n",
       "6     9.0    3.0     1428      34.0   72.0  44.0   3.0     -7.0       1.0   \n",
       "7     9.0    3.0     1830      13.0   -9.0  46.0   4.0     -7.0       1.0   \n",
       "\n",
       "   SEX  ...  RX30V3C2  RX30V3C3  RX30V3C4  SETTYPE    YEAR     CSTRATM  \\\n",
       "0  1.0  ...       NaN       NaN       NaN      3.0  2022.0  20122201.0   \n",
       "1  2.0  ...       NaN       NaN       NaN      3.0  2022.0  20122201.0   \n",
       "2  2.0  ...       NaN       NaN       NaN      3.0  2022.0  20122201.0   \n",
       "3  2.0  ...       NaN       NaN       NaN      3.0  2022.0  20122201.0   \n",
       "4  1.0  ...       NaN       NaN       NaN      3.0  2022.0  20122201.0   \n",
       "5  2.0  ...       NaN       NaN       NaN      3.0  2022.0  20122201.0   \n",
       "6  1.0  ...       NaN       NaN       NaN      3.0  2022.0  20122201.0   \n",
       "7  2.0  ...       NaN       NaN       NaN      3.0  2022.0  20122201.0   \n",
       "\n",
       "      CPSUM       PATWT     EDWT  BOARDED  \n",
       "0  100001.0  3665.56954  8.36413     -7.0  \n",
       "1  100001.0  3665.56954      NaN     -7.0  \n",
       "2  100001.0  3665.56954      NaN     -7.0  \n",
       "3  100001.0  3665.56954      NaN     -7.0  \n",
       "4  100001.0  3665.56954      NaN     -7.0  \n",
       "5  100001.0  3665.56954      NaN     -7.0  \n",
       "6  100001.0  3665.56954      NaN     -7.0  \n",
       "7  100001.0  3665.56954      NaN     -7.0  \n",
       "\n",
       "[8 rows x 913 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(16025, 913)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "display(df.head(8))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb185c0-732c-4e0e-aa76-4a9b96d48b3e",
   "metadata": {},
   "source": [
    "## 1.1 Limpieza mínima y variables derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0150375e-a843-48b5-8d21-50fc051567ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARRTIME</th>\n",
       "      <th>ARRTIME_ts</th>\n",
       "      <th>ARR_HOUR</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGEDAYS</th>\n",
       "      <th>AGE_YEARS</th>\n",
       "      <th>WAITTIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>604.0</td>\n",
       "      <td>2022-01-01 06:04:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1053.0</td>\n",
       "      <td>2022-01-01 10:53:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1419.0</td>\n",
       "      <td>2022-01-01 14:19:00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1825.0</td>\n",
       "      <td>2022-01-01 18:25:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>0.81588</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2243.0</td>\n",
       "      <td>2022-01-01 22:43:00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>903.0</td>\n",
       "      <td>2022-01-01 09:03:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.00000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1428.0</td>\n",
       "      <td>2022-01-01 14:28:00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.00000</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1830.0</td>\n",
       "      <td>2022-01-01 18:30:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.00000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ARRTIME          ARRTIME_ts  ARR_HOUR   AGE  AGEDAYS  AGE_YEARS  WAITTIME\n",
       "0    604.0 2022-01-01 06:04:00       6.0  23.0      NaN   23.00000      10.0\n",
       "1   1053.0 2022-01-01 10:53:00      10.0  15.0      NaN   15.00000      40.0\n",
       "2   1419.0 2022-01-01 14:19:00      14.0  19.0      NaN   19.00000      70.0\n",
       "3   1825.0 2022-01-01 18:25:00      18.0   0.0    298.0    0.81588       NaN\n",
       "4   2243.0 2022-01-01 22:43:00      22.0  18.0      NaN   18.00000      14.0\n",
       "5    903.0 2022-01-01 09:03:00       9.0  39.0      NaN   39.00000       2.0\n",
       "6   1428.0 2022-01-01 14:28:00      14.0  44.0      NaN   44.00000      34.0\n",
       "7   1830.0 2022-01-01 18:30:00      18.0  46.0      NaN   46.00000      13.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(16025, 916)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# A) Mapear códigos de no-respuesta a NaN (SAS usa -7/-8/-9)\n",
    "na_codes = {-7: np.nan, -8: np.nan, -9: np.nan}\n",
    "df = df.replace(na_codes)\n",
    "\n",
    "# B) ARRTIME (HHMM) -> timestamp con fecha dummy y hora de llegada\n",
    "def hhmm_to_ts(x):\n",
    "    if pd.isna(x): \n",
    "        return pd.NaT\n",
    "    try:\n",
    "        x = int(x)\n",
    "        h, m = divmod(x, 100)\n",
    "        if 0 <= h <= 23 and 0 <= m <= 59:\n",
    "            return pd.Timestamp(2022, 1, 1, h, m)  # fecha dummy\n",
    "    except Exception:\n",
    "        pass\n",
    "    return pd.NaT\n",
    "\n",
    "if \"ARRTIME\" in df.columns:\n",
    "    df[\"ARRTIME_ts\"] = df[\"ARRTIME\"].apply(hhmm_to_ts)\n",
    "    df[\"ARR_HOUR\"] = df[\"ARRTIME_ts\"].dt.hour\n",
    "\n",
    "# C) Edad en años (si AGE==0 usar AGEDAYS/365.25 cuando esté disponible)\n",
    "if {\"AGE\",\"AGEDAYS\"}.issubset(df.columns):\n",
    "    df[\"AGE_YEARS\"] = df[\"AGE\"]\n",
    "    mask_infant = (df[\"AGE_YEARS\"] == 0) & df[\"AGEDAYS\"].notna()\n",
    "    df.loc[mask_infant, \"AGE_YEARS\"] = df.loc[mask_infant, \"AGEDAYS\"] / 365.25\n",
    "\n",
    "# D) WAITTIME negativo -> NaN (por seguridad)\n",
    "if \"WAITTIME\" in df.columns:\n",
    "    df.loc[df[\"WAITTIME\"] < 0, \"WAITTIME\"] = np.nan\n",
    "\n",
    "# Vista rápida tras la limpieza\n",
    "display(df[[\"ARRTIME\",\"ARRTIME_ts\",\"ARR_HOUR\",\"AGE\",\"AGEDAYS\",\"AGE_YEARS\",\"WAITTIME\"]]\n",
    "        .head(8)\n",
    "        .pipe(lambda x: x[[c for c in x.columns if c in df.columns]]))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a1fe2-0b18-43ea-bcf3-069704ef3c1e",
   "metadata": {},
   "source": [
    "## 1.2 Guardar limpio y actualizar ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cae0132-ec74-4e09-8f69-b36db5afbeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\ed2022_clean.csv\n"
     ]
    }
   ],
   "source": [
    "CLEAN_PATH = CSV_PATH.with_name(\"ed2022_clean.csv\")\n",
    "df.to_csv(CLEAN_PATH, index=False, encoding=\"utf-8\")\n",
    "print(\"Guardado:\", CLEAN_PATH)\n",
    "\n",
    "# desde acá, toda la doc usa el CSV limpio\n",
    "CSV_PATH = CLEAN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03321c20-a693-4894-a66e-9cb5fd537842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV limpio: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\ed2022_clean.csv\n",
      "Tamaño (MB): 27.26\n",
      "SHA-256: 3c7a42ba08a94bc22c1759495ef7050bef7c021073a07c6adfa008be0af15a9a\n"
     ]
    }
   ],
   "source": [
    "import os, hashlib\n",
    "def sha256(p):\n",
    "    h = hashlib.sha256()\n",
    "    with open(p, \"rb\") as f:\n",
    "        for c in iter(lambda: f.read(1<<20), b\"\"): h.update(c)\n",
    "    return h.hexdigest()\n",
    "\n",
    "print(\"CSV limpio:\", CSV_PATH)\n",
    "print(\"Tamaño (MB):\", round(os.path.getsize(CSV_PATH)/1024**2, 2))\n",
    "print(\"SHA-256:\", sha256(CSV_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8429c2d",
   "metadata": {},
   "source": [
    "## 2 Tipos de datos y uso de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f503fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtype</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>float64</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>object</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dtype  count\n",
       "0         float64    872\n",
       "1          object     43\n",
       "2  datetime64[ns]      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(132.58028316497803)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "types = df.dtypes.astype(str).value_counts().rename_axis('dtype').reset_index(name='count')\n",
    "display(types)\n",
    "mem_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d2a9f3",
   "metadata": {},
   "source": [
    "## 3 Perfil por columna (faltantes, cardinalidad, ejemplos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d3261eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>label</th>\n",
       "      <th>dtype</th>\n",
       "      <th>semantic_type</th>\n",
       "      <th>non_null</th>\n",
       "      <th>missing_pct</th>\n",
       "      <th>unique</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>example_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VMONTH</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0, 10.0, 8.0, 11.0, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VDAYR</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0, 3.0, 4.0, 5.0, 6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARRTIME</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>15782</td>\n",
       "      <td>1.5164</td>\n",
       "      <td>1437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2359.0</td>\n",
       "      <td>604.0, 1053.0, 1419.0, 1825.0, 2243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WAITTIME</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>13272</td>\n",
       "      <td>17.1794</td>\n",
       "      <td>396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>10.0, 40.0, 70.0, 14.0, 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOV</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>15328</td>\n",
       "      <td>4.3495</td>\n",
       "      <td>1435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5722.0</td>\n",
       "      <td>228.0, 319.0, 551.0, 168.0, 113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AGE</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>23.0, 15.0, 19.0, 0.0, 18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AGER</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0, 1.0, 3.0, 4.0, 5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AGEDAYS</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>428</td>\n",
       "      <td>97.3292</td>\n",
       "      <td>243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>298.0, 198.0, 153.0, 351.0, 55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RESIDNCE</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>15763</td>\n",
       "      <td>1.6349</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0, 3.0, 4.0, 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SEX</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0, 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ETHUN</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>13692</td>\n",
       "      <td>14.5585</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ETHIM</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RACEUN</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>13003</td>\n",
       "      <td>18.8580</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0, 3.0, 2.0, 6.0, 5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RACER</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0, 3.0, 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RACERETH</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0, 4.0, 3.0, 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ARREMS</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>15530</td>\n",
       "      <td>3.0889</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AMBTRANSFER</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>2388</td>\n",
       "      <td>85.0983</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NOPAY</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PAYPRIV</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0, 1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PAYMCARE</td>\n",
       "      <td></td>\n",
       "      <td>float64</td>\n",
       "      <td>numeric</td>\n",
       "      <td>16025</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0, 1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       variable label    dtype semantic_type  non_null  missing_pct  unique  \\\n",
       "0        VMONTH        float64       numeric     16025       0.0000      12   \n",
       "1         VDAYR        float64       numeric     16025       0.0000       7   \n",
       "2       ARRTIME        float64       numeric     15782       1.5164    1437   \n",
       "3      WAITTIME        float64       numeric     13272      17.1794     396   \n",
       "4           LOV        float64       numeric     15328       4.3495    1435   \n",
       "5           AGE        float64       numeric     16025       0.0000      95   \n",
       "6          AGER        float64       numeric     16025       0.0000       6   \n",
       "7       AGEDAYS        float64       numeric       428      97.3292     243   \n",
       "8      RESIDNCE        float64       numeric     15763       1.6349       4   \n",
       "9           SEX        float64       numeric     16025       0.0000       2   \n",
       "10        ETHUN        float64       numeric     13692      14.5585       2   \n",
       "11        ETHIM        float64       numeric     16025       0.0000       2   \n",
       "12       RACEUN        float64       numeric     13003      18.8580       6   \n",
       "13        RACER        float64       numeric     16025       0.0000       3   \n",
       "14     RACERETH        float64       numeric     16025       0.0000       4   \n",
       "15       ARREMS        float64       numeric     15530       3.0889       2   \n",
       "16  AMBTRANSFER        float64       numeric      2388      85.0983       2   \n",
       "17        NOPAY        float64       numeric     16025       0.0000       2   \n",
       "18      PAYPRIV        float64       numeric     16025       0.0000       2   \n",
       "19     PAYMCARE        float64       numeric     16025       0.0000       2   \n",
       "\n",
       "    min     max                         example_values  \n",
       "0   1.0    12.0              9.0, 10.0, 8.0, 11.0, 1.0  \n",
       "1   1.0     7.0                2.0, 3.0, 4.0, 5.0, 6.0  \n",
       "2   0.0  2359.0  604.0, 1053.0, 1419.0, 1825.0, 2243.0  \n",
       "3   0.0  1280.0            10.0, 40.0, 70.0, 14.0, 2.0  \n",
       "4   0.0  5722.0      228.0, 319.0, 551.0, 168.0, 113.0  \n",
       "5   0.0    94.0            23.0, 15.0, 19.0, 0.0, 18.0  \n",
       "6   1.0     6.0                2.0, 1.0, 3.0, 4.0, 5.0  \n",
       "7   1.0   364.0       298.0, 198.0, 153.0, 351.0, 55.0  \n",
       "8   1.0     4.0                     1.0, 3.0, 4.0, 2.0  \n",
       "9   1.0     2.0                               1.0, 2.0  \n",
       "10  1.0     2.0                               2.0, 1.0  \n",
       "11  1.0     2.0                               2.0, 1.0  \n",
       "12  1.0     6.0                1.0, 3.0, 2.0, 6.0, 5.0  \n",
       "13  1.0     3.0                          1.0, 3.0, 2.0  \n",
       "14  1.0     4.0                     1.0, 4.0, 3.0, 2.0  \n",
       "15  1.0     2.0                               2.0, 1.0  \n",
       "16  1.0     2.0                               2.0, 1.0  \n",
       "17  0.0     1.0                               0.0, 1.0  \n",
       "18  0.0     1.0                               0.0, 1.0  \n",
       "19  0.0     1.0                               0.0, 1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(916, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Perfil por columna (corregido, sin infer_datetime_format)\n",
    "from textwrap import shorten\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "assert 'df' in globals(), \"df no definido (corré la carga antes)\"\n",
    "\n",
    "def sample_vals(series, n=5):\n",
    "    vals = pd.unique(series.dropna().head(1000))[:n]\n",
    "    return \", \".join([shorten(str(v), width=40, placeholder=\"…\") for v in vals])\n",
    "\n",
    "# Heurística liviana para detectar fechas en columnas 'object'\n",
    "_date_name_pat  = re.compile(r\"(date|fecha|time|hour|_ts|_dt)$\", re.I)\n",
    "_date_token_pat = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}|^\\d{2}/\\d{2}/\\d{4}|^\\d{4}/\\d{2}/\\d{2}\")\n",
    "\n",
    "rows = []\n",
    "n_rows = len(df)\n",
    "\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    base_dtype = s.dtype\n",
    "\n",
    "    # Tipo semántico base\n",
    "    if np.issubdtype(base_dtype, np.number):\n",
    "        sem_type = \"numeric\"\n",
    "    elif np.issubdtype(base_dtype, np.datetime64):\n",
    "        sem_type = \"datetime\"\n",
    "    else:\n",
    "        looks_date = False\n",
    "        if base_dtype == \"object\":\n",
    "            head_vals = s.dropna().astype(str).head(50)\n",
    "            # por nombre o por tokens con forma de fecha\n",
    "            looks_date = bool(_date_name_pat.search(c)) or (head_vals.str.match(_date_token_pat).mean() > 0.6)\n",
    "        if looks_date:\n",
    "            # <— sin infer_datetime_format\n",
    "            parsed = pd.to_datetime(s, errors=\"coerce\")\n",
    "            if parsed.notna().sum() / len(s) > 0.8:\n",
    "                sem_type = \"datetime\"\n",
    "                s = parsed\n",
    "            else:\n",
    "                sem_type = \"categorical/text\"\n",
    "        else:\n",
    "            sem_type = \"categorical/text\"\n",
    "\n",
    "    non_null = int(s.notna().sum())\n",
    "    missing_pct = (1 - non_null / n_rows) * 100 if n_rows else 0.0\n",
    "    nunique = int(s.nunique(dropna=True))\n",
    "\n",
    "    nmin = nmax = \"\"\n",
    "    if sem_type == \"numeric\":\n",
    "        sn = pd.to_numeric(s, errors=\"coerce\")\n",
    "        if sn.notna().any():\n",
    "            nmin = float(sn.min())\n",
    "            nmax = float(sn.max())\n",
    "\n",
    "    rows.append({\n",
    "        \"variable\": c,\n",
    "        \"label\": \"\",\n",
    "        \"dtype\": str(base_dtype),\n",
    "        \"semantic_type\": sem_type,\n",
    "        \"non_null\": non_null,\n",
    "        \"missing_pct\": round(missing_pct, 4),\n",
    "        \"unique\": nunique,\n",
    "        \"min\": nmin,\n",
    "        \"max\": nmax,\n",
    "        \"example_values\": sample_vals(s, n=5),\n",
    "    })\n",
    "\n",
    "dict_df = pd.DataFrame(rows)\n",
    "\n",
    "# Vista rápida (opcional)\n",
    "display(dict_df.head(20))\n",
    "dict_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd98935-4b6f-4411-ab65-f4bc2110b0f9",
   "metadata": {},
   "source": [
    "## 3.1 Depuración mínima + listado + CSV “modelable”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11795d04-2df1-416e-a7c0-37093c75fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listado guardado en: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\\dropped_columns.csv\n",
      "Columnas 100% NaN: 72\n",
      "Columnas con 1 valor: 72\n",
      "Total a eliminar: 144\n",
      "Guardado: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\ed2022_clean_min.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16025, 771)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depuración mínima a partir de dict_df\n",
    "\n",
    "# A) columnas 100% NaN o con un solo valor\n",
    "cols_all_nan   = dict_df.query(\"missing_pct == 100.0\")[\"variable\"].tolist()\n",
    "cols_singleval = dict_df.query(\"unique == 1\")[\"variable\"].tolist()\n",
    "drop_cols = sorted(set(cols_all_nan + cols_singleval))\n",
    "\n",
    "# Guardar listado de columnas removidas (para anexar)\n",
    "(OUT_DIR / \"dropped_columns.csv\").parent.mkdir(exist_ok=True, parents=True)\n",
    "pd.Series(drop_cols, name=\"dropped_columns\").to_csv(\n",
    "    OUT_DIR / \"dropped_columns.csv\", index=False, encoding=\"utf-8\"\n",
    ")\n",
    "print(\"Listado guardado en:\", OUT_DIR / \"dropped_columns.csv\")\n",
    "\n",
    "print(f\"Columnas 100% NaN: {len(cols_all_nan)}\")\n",
    "print(f\"Columnas con 1 valor: {len(cols_singleval)}\")\n",
    "print(f\"Total a eliminar: {len(drop_cols)}\")\n",
    "\n",
    "# B) aplicar drops\n",
    "df_ml = df.drop(columns=drop_cols, errors=\"ignore\").copy()\n",
    "\n",
    "# C) quitar ARRTIME (redundante) si existe\n",
    "if \"ARRTIME\" in df_ml.columns:\n",
    "    df_ml = df_ml.drop(columns=[\"ARRTIME\"])\n",
    "\n",
    "# D) guardar CSV “modelable” y actualizar variables para el resto del notebook\n",
    "CLEAN_MIN_PATH = CSV_PATH.with_name(\"ed2022_clean_min.csv\")\n",
    "df_ml.to_csv(CLEAN_MIN_PATH, index=False, encoding=\"utf-8\")\n",
    "print(\"Guardado:\", CLEAN_MIN_PATH)\n",
    "\n",
    "# Usar este DF y ruta desde ahora\n",
    "df = df_ml\n",
    "CSV_PATH = CLEAN_MIN_PATH\n",
    "\n",
    "# Evidencia rápida\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b1148-1550-4966-a6fa-859a1ce0a576",
   "metadata": {},
   "source": [
    "## 3.2 — Validar que dropped_columns.csv solo tenga columnas vacías/constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449c1086-1afb-4a6c-aaa6-8ebb33f4d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: dropped_columns.csv solo contiene 100% NaN o constantes.\n"
     ]
    }
   ],
   "source": [
    "# Validación rápida de dropped_columns.csv respecto de dict_df\n",
    "dropped = pd.read_csv(OUT_DIR / \"dropped_columns.csv\")[\"dropped_columns\"].tolist()\n",
    "df_sub = dict_df.set_index(\"variable\").loc[dropped][[\"missing_pct\",\"unique\"]]\n",
    "bad = df_sub.query(\"missing_pct < 100.0 and unique > 1\")\n",
    "if len(bad):\n",
    "    print(\"⚠️ Hay columnas en dropped_columns.csv que no son 100% NaN ni constantes:\")\n",
    "    display(bad)\n",
    "else:\n",
    "    print(\"OK: dropped_columns.csv solo contiene 100% NaN o constantes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09aa4e6",
   "metadata": {},
   "source": [
    "## 4 Generación de documentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213286b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reescrito: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\\DATADICT.md | Partes: 4 | CSV: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\\DATADICT_full.csv\n",
      "Reescrito: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\\DATASET.md\n",
      "Reescrito: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\\PROVENANCE.md\n",
      "SHA256SUMS actualizado -> D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\\SHA256SUMS.txt\n"
     ]
    }
   ],
   "source": [
    "# 4 — Generación de documentación (único bloque, CONSOLIDA todo)\n",
    "from textwrap import shorten\n",
    "import pandas as pd, numpy as np, re, math\n",
    "from pandas.api.types import is_object_dtype, is_datetime64_any_dtype, is_numeric_dtype\n",
    "from pathlib import Path\n",
    "import os, hashlib\n",
    "\n",
    "# --- seguridad: df y rutas existentes\n",
    "assert 'df' in globals(), \"df no definido (corré 1→3.1 antes)\"\n",
    "assert 'CSV_PATH' in globals() and Path(CSV_PATH).exists(), \"CSV_PATH no existe\"\n",
    "assert 'OUT_DIR' in globals() and Path(OUT_DIR).exists(), \"OUT_DIR no existe\"\n",
    "assert 'ARRTIME' not in df.columns, \"ARRTIME sigue presente (corré 3.1)\"\n",
    "\n",
    "# --- C: métricas del CSV actual\n",
    "def sha256_of_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "CSV_PATH = Path(CSV_PATH)\n",
    "CSV_MB   = CSV_PATH.stat().st_size/(1024**2)\n",
    "MEM_MB   = df.memory_usage(deep=True).sum()/(1024**2)\n",
    "CSV_SHA  = sha256_of_file(CSV_PATH)\n",
    "\n",
    "# --- B (recalculado sobre DF limpio): perfil por columna (dict_df) sin infer_datetime_format\n",
    "def sample_vals(series, n=5):\n",
    "    vals = pd.unique(series.dropna().head(1000))[:n]\n",
    "    return \", \".join([shorten(str(v), width=40, placeholder=\"…\") for v in vals])\n",
    "\n",
    "_date_name_pat  = re.compile(r\"(date|fecha|time|hour|_ts|_dt)$\", re.I)\n",
    "_date_token_pat = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}|^\\d{2}/\\d{2}/\\d{4}|^\\d{4}/\\d{2}/\\d{2}\")\n",
    "\n",
    "rows = []\n",
    "n_rows = len(df)\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    base_dtype = s.dtype\n",
    "\n",
    "    if is_numeric_dtype(base_dtype):\n",
    "        sem_type = \"numeric\"\n",
    "    elif is_datetime64_any_dtype(base_dtype):\n",
    "        sem_type = \"datetime\"\n",
    "    else:\n",
    "        if is_object_dtype(base_dtype):\n",
    "            head_vals = s.dropna().astype(str).head(50)\n",
    "            looks_date = bool(_date_name_pat.search(c)) or (head_vals.str.match(_date_token_pat).mean() > 0.6)\n",
    "        else:\n",
    "            looks_date = False\n",
    "        if looks_date:\n",
    "            parsed = pd.to_datetime(s, errors=\"coerce\")\n",
    "            if parsed.notna().sum() / len(s) > 0.8:\n",
    "                sem_type = \"datetime\"\n",
    "                s = parsed\n",
    "            else:\n",
    "                sem_type = \"categorical/text\"\n",
    "        else:\n",
    "            sem_type = \"categorical/text\"\n",
    "\n",
    "    non_null = int(s.notna().sum())\n",
    "    missing_pct = (1 - non_null / n_rows) * 100 if n_rows else 0.0\n",
    "    nunique = int(s.nunique(dropna=True))\n",
    "\n",
    "    nmin = nmax = \"\"\n",
    "    if sem_type == \"numeric\":\n",
    "        sn = pd.to_numeric(s, errors=\"coerce\")\n",
    "        if sn.notna().any():\n",
    "            nmin = float(sn.min()); nmax = float(sn.max())\n",
    "\n",
    "    rows.append({\n",
    "        \"variable\": c, \"label\": \"\", \"dtype\": str(base_dtype), \"semantic_type\": sem_type,\n",
    "        \"non_null\": non_null, \"missing_pct\": round(missing_pct, 4), \"unique\": nunique,\n",
    "        \"min\": nmin, \"max\": nmax, \"example_values\": sample_vals(s, n=5)\n",
    "    })\n",
    "\n",
    "dict_df = pd.DataFrame(rows)\n",
    "\n",
    "# --- D: DATADICT_full.csv + partes Markdown + índice\n",
    "def _san(s):\n",
    "    s = \"\" if pd.isna(s) else str(s)\n",
    "    return s.replace(\"|\", r\"\\|\").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "dd = dict_df.copy()\n",
    "(dd_path := OUT_DIR / \"DATADICT_full.csv\").write_text(dd.to_csv(index=False), encoding=\"utf-8\")\n",
    "\n",
    "rows_per_file = 200\n",
    "n_parts = math.ceil(len(dd) / rows_per_file)\n",
    "header = (\n",
    "    \"| variable | label | dtype | semantic_type | non_null | missing_pct | unique | min | max | example_values |\\n\"\n",
    "    \"|---|---|---|---|---:|---:|---:|---:|---:|---|\\n\"\n",
    ")\n",
    "\n",
    "index_lines = [\n",
    "    \"# Diccionario de Datos — NHAMCS ED 2022 (CSV depurado)\",\n",
    "    f\"- **Archivo**: `{CSV_PATH.name}` | **Tamaño**: {CSV_MB:.2f} MB | **SHA256**: `{CSV_SHA}`\",\n",
    "    f\"- **Filas**: **{len(df)}** | **Columnas**: **{df.shape[1]}** | **Memoria (pandas)**: **{MEM_MB:.2f} MB**\",\n",
    "    \"\", \"**Vista completa (CSV)**: `DATADICT_full.csv`\",\n",
    "    f\"**Partes Markdown** ({n_parts} archivos de ~{rows_per_file} filas):\"\n",
    "]\n",
    "for i in range(n_parts):\n",
    "    start = i*rows_per_file; end = min((i+1)*rows_per_file, len(dd))\n",
    "    part_path = OUT_DIR / f\"DATADICT_part{i+1}.md\"\n",
    "    part_lines = [header]\n",
    "    for _, r in dd.iloc[start:end].iterrows():\n",
    "        part_lines.append(\n",
    "            f\"| {_san(r['variable'])} | {_san(r.get('label',''))} | {_san(r['dtype'])} | {_san(r['semantic_type'])} | \"\n",
    "            f\"{int(r['non_null'])} | {r['missing_pct']:.4f}% | {int(r['unique'])} | {_san(r['min'])} | {_san(r['max'])} | {_san(r['example_values'])} |\"\n",
    "        )\n",
    "    part_path.write_text(\"\\n\".join(part_lines), encoding=\"utf-8\")\n",
    "    index_lines.append(f\"- `DATADICT_part{i+1}.md` (filas {start+1}–{end})\")\n",
    "\n",
    "(datadict_md := OUT_DIR / \"DATADICT.md\").write_text(\"\\n\".join(index_lines), encoding=\"utf-8\")\n",
    "print(\"Reescrito:\", datadict_md, \"| Partes:\", n_parts, \"| CSV:\", dd_path)\n",
    "\n",
    "# --- E: DATASET.md (coherente con df limpio)  <<-- aquí está el cambio solicitado\n",
    "num_numeric = int((dict_df[\"semantic_type\"]==\"numeric\").sum())\n",
    "num_dt      = int((dict_df[\"semantic_type\"]==\"datetime\").sum())\n",
    "num_cat     = int((dict_df[\"semantic_type\"]==\"categorical/text\").sum())\n",
    "\n",
    "dataset_md = OUT_DIR / \"DATASET.md\"\n",
    "with open(dataset_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# DATASET.md — Descripción técnica del dataset\\n\\n\")\n",
    "    f.write(\"**Nombre**: NHAMCS — Emergency Department 2022 (CSV depurado).  \\n\")\n",
    "    f.write(f\"**Archivo**: `{CSV_PATH.name}`  \\n\")\n",
    "    f.write(f\"**Filas (instancias)**: **{len(df)}**  \\n\")                 # <- texto exacto para el validador\n",
    "    f.write(f\"**Columnas (características)**: **{df.shape[1]}**  \\n\")    # <- texto exacto para el validador\n",
    "    f.write(f\"**Tamaño en disco (CSV)**: **{CSV_MB:.2f} MB**  \\n\")\n",
    "    f.write(f\"**Uso de memoria aprox. (pandas)**: **{MEM_MB:.2f} MB**  \\n\")\n",
    "    f.write(f\"**SHA256 (CSV)**: `{CSV_SHA}`\\n\\n\")                        # <- formato exacto para el validador\n",
    "    f.write(\"## Tipos de datos (inferidos por pandas)\\n\")\n",
    "    f.write(f\"- Numéricas: {num_numeric}\\n\")\n",
    "    f.write(f\"- Fechas/horas: {num_dt}\\n\")\n",
    "    f.write(f\"- Categóricas/Textuales: {num_cat}\\n\\n\")\n",
    "    f.write(\"## Calidad inicial (resumen)\\n\")\n",
    "    try:\n",
    "        f.write(f\"- **Depuración aplicada:** {len(cols_all_nan)} columnas 100% NaN y {len(cols_singleval)} constantes fueron removidas; ver `dropped_columns.csv`.\\n\")\n",
    "    except NameError:\n",
    "        f.write(\"- **Depuración aplicada:** se removieron columnas 100% NaN y constantes; ver `dropped_columns.csv`.\\n\")\n",
    "    f.write(f\"- Columnas con faltantes (>0%): {(dict_df['missing_pct']>0).sum()} / {df.shape[1]}\\n\")\n",
    "    f.write(f\"- Columnas sin faltantes: {(dict_df['missing_pct']==0).sum()}\\n\")\n",
    "    f.write(f\"- Columnas con alta cardinalidad (unique > 500): {(dict_df['unique']>500).sum()}\\n\\n\")\n",
    "    f.write(\"> Nota: Los **labels/definiciones** originales de SAS no vienen en el CSV. Deben completarse con el **diccionario oficial NHAMCS 2022**.\\n\\n\")\n",
    "    f.write(\"Para el detalle por columna, ver **DATADICT.md**.\\n\")\n",
    "print(\"Reescrito:\", dataset_md)\n",
    "\n",
    "# --- F: PROVENANCE.md\n",
    "prov_md = OUT_DIR / \"PROVENANCE.md\"\n",
    "with open(prov_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# PROVENANCE.md — Origen y trazabilidad\\n\\n\")\n",
    "    f.write(\"- **Fuente original**: archivo SAS NHAMCS ED 2022 (`ed2022_sas.sas7bdat`).  \\n\")\n",
    "    f.write(\"- **Conversión**: SAS → CSV realizada en Jupyter con `pyreadstat` + `pandas`.  \\n\")\n",
    "    f.write(f\"- **Archivo actual**: `{CSV_PATH.name}` | **Tamaño**: {CSV_MB:.2f} MB | **SHA256**: `{CSV_SHA}`  \\n\")\n",
    "    f.write(f\"- **Fecha de conversión**: {pd.Timestamp.today().date()}  \\n\")\n",
    "    f.write(\"- **Licencia/uso**: Uso académico – NHAMCS/CDC 2022 (ver condiciones de NCHS/CDC).\\n\\n\")\n",
    "    f.write(\"## Pasos reproducibles\\n\")\n",
    "    f.write(\"1. `df, meta = pyreadstat.read_sas7bdat('ed2022_sas.sas7bdat')`\\n\")\n",
    "    f.write(\"2. Limpieza mínima: códigos -7/-8/-9 → NaN; `ARRTIME` → `ARRTIME_ts`/`ARR_HOUR`; `WAITTIME<0` → NaN.\\n\")\n",
    "    f.write(\"3. Depuración: eliminación de columnas 100% NaN y constantes; remoción de `ARRTIME` (redundante).\\n\")\n",
    "    f.write(f\"4. `df.to_csv('{CSV_PATH.name}', index=False, encoding='utf-8')`\\n\")\n",
    "print(\"Reescrito:\", prov_md)\n",
    "\n",
    "# --- G: SHA256SUMS.txt (UNA sola vez)\n",
    "sha_txt = OUT_DIR / \"SHA256SUMS.txt\"\n",
    "sha_txt.write_text(f\"{CSV_SHA}  {CSV_PATH.name}\\n\", encoding=\"utf-8\")\n",
    "print(\"SHA256SUMS actualizado ->\", sha_txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79f989-6a2d-46a1-bd3f-9dc8d297b216",
   "metadata": {},
   "source": [
    "## 4.1 Normalizar encabezado de DATADICT.md (hotfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa23275e-14f1-43bf-bdd9-b6c8915360b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DATADICT.md normalizado (encabezado con Filas/Columnas/SHA en líneas separadas).\n"
     ]
    }
   ],
   "source": [
    "# --- Hotfix: normalizar encabezado de DATADICT.md para el validador ---\n",
    "from pathlib import Path\n",
    "import os, hashlib\n",
    "\n",
    "# Rutas que ya venís usando\n",
    "BASE = Path.cwd()\n",
    "OUT_DIR  = BASE / \"dataset_docs\"\n",
    "DATA_DIR = BASE / \"data\"\n",
    "CSV_PATH = DATA_DIR / \"ed2022_clean_min.csv\"   # ajustá si usás otro CSV\n",
    "\n",
    "assert OUT_DIR.exists(), \"OUT_DIR no existe\"\n",
    "assert CSV_PATH.exists(), f\"No existe el CSV en {CSV_PATH}\"\n",
    "assert 'df' in globals(), \"df no definido (corré las celdas de carga antes)\"\n",
    "\n",
    "def sha256_of_file(p: Path) -> str:\n",
    "    import hashlib\n",
    "    h = hashlib.sha256()\n",
    "    with open(p, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# Métricas actuales\n",
    "CSV_MB  = CSV_PATH.stat().st_size / (1024**2)\n",
    "CSV_SHA = sha256_of_file(CSV_PATH)\n",
    "try:\n",
    "    MEM_MB = df.memory_usage(deep=True).sum()/(1024**2)\n",
    "except Exception:\n",
    "    MEM_MB = float(\"nan\")\n",
    "\n",
    "# Detectar partes existentes para listarlas\n",
    "parts = sorted(OUT_DIR.glob(\"DATADICT_part*.md\"))\n",
    "\n",
    "# Encabezado NORMALIZADO (líneas simples que el validador busca)\n",
    "index_lines = [\n",
    "    \"# Diccionario de Datos — NHAMCS ED 2022 (CSV depurado)\",\n",
    "    f\"- **Archivo**: `{CSV_PATH.name}`\",\n",
    "    f\"- **Filas**: **{len(df)}**\",\n",
    "    f\"- **Columnas**: **{df.shape[1]}**\",\n",
    "    f\"- **Tamaño**: {CSV_MB:.2f} MB\",\n",
    "    f\"- **Memoria (pandas)**: **{MEM_MB:.2f} MB**\",\n",
    "    f\"- **SHA256**: {CSV_SHA}\",\n",
    "    \"\",\n",
    "    \"**Vista completa (CSV)**: `DATADICT_full.csv`\",\n",
    "    f\"**Partes Markdown** ({len(parts)} archivos):\",\n",
    "]\n",
    "index_lines += [f\"- `{p.name}`\" for p in parts]\n",
    "\n",
    "# Reescribir SOLO el índice (no toca DATADICT_full.csv ni las partes)\n",
    "(OUT_DIR / \"DATADICT.md\").write_text(\"\\n\".join(index_lines), encoding=\"utf-8\")\n",
    "print(\"✅ DATADICT.md normalizado (encabezado con Filas/Columnas/SHA en líneas separadas).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed226a-0593-43d0-a273-b901594c0086",
   "metadata": {},
   "source": [
    "## 4.2 Empaquetado para entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58ed2afb-f101-4f43-a966-b2e6abc0c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP generado: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\\DATADICT_bundle.zip | contiene 6 archivos\n",
      "Muestra generada: D:\\ALMACENAMIENTO\\VARIOS\\FACULTAD\\aprendizaje_automatico\\Examen_parcial_proyecto\\Notebooks\\2. Documentacion_dataset\\dataset_docs\\DATADICT_sample.md\n"
     ]
    }
   ],
   "source": [
    "# === Empaquetar y generar versión liviana del DATADICT ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, zipfile, io\n",
    "\n",
    "OUT_DIR = Path(OUT_DIR)  # ya lo tenés definido\n",
    "idx_md   = OUT_DIR / \"DATADICT.md\"\n",
    "full_csv = OUT_DIR / \"DATADICT_full.csv\"\n",
    "parts    = sorted(OUT_DIR.glob(\"DATADICT_part*.md\"))\n",
    "\n",
    "# 1) ZIP con todo lo necesario para revisar\n",
    "zip_path = OUT_DIR / \"DATADICT_bundle.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    if idx_md.exists():   z.write(idx_md, arcname=idx_md.name)\n",
    "    if full_csv.exists(): z.write(full_csv, arcname=full_csv.name)\n",
    "    for p in parts: z.write(p, arcname=p.name)\n",
    "print(\"ZIP generado:\", zip_path, \"| contiene\", len(parts)+int(idx_md.exists())+int(full_csv.exists()), \"archivos\")\n",
    "\n",
    "# 2) MUESTRA liviana (primeras 60 filas) en un único .md\n",
    "df_full = pd.read_csv(full_csv)\n",
    "m = df_full.head(60)\n",
    "cols = [\"variable\",\"label\",\"dtype\",\"semantic_type\",\"non_null\",\"missing_pct\",\"unique\",\"min\",\"max\",\"example_values\"]\n",
    "m = m[[c for c in cols if c in m.columns]]\n",
    "\n",
    "def san(x):\n",
    "    x = \"\" if pd.isna(x) else str(x)\n",
    "    return x.replace(\"|\", r\"\\|\").replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "\n",
    "lines = [\n",
    "    \"# DATADICT_sample.md — primeras 60 filas\",\n",
    "    \"\",\n",
    "    \"| \" + \" | \".join(m.columns) + \" |\",\n",
    "    \"|\"+ \"|\".join([\"---\"]*len(m.columns)) + \"|\",\n",
    "]\n",
    "for _, r in m.iterrows():\n",
    "    lines.append(\"| \" + \" | \".join(san(r[c]) for c in m.columns) + \" |\")\n",
    "\n",
    "sample_md = OUT_DIR / \"DATADICT_sample.md\"\n",
    "sample_md.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "print(\"Muestra generada:\", sample_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd504969-b8a5-466b-9b93-77e067b7fab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DATADICT.md', 'DATADICT_bundle.zip', 'DATADICT_full.csv', 'DATADICT_part1.md', 'DATADICT_part2.md', 'DATADICT_part3.md', 'DATADICT_part4.md', 'DATADICT_sample.md', 'DATASET.md', 'dropped_columns.csv', 'PROVENANCE.md', 'SHA256SUMS.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(OUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f434479-0d10-4981-804b-6e7429f9bd3c",
   "metadata": {},
   "source": [
    "## 4.3 Validador express de documentación y archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e92c2a36-cc8d-4e0d-be2c-2814b9e8348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VALIDADOR: TODO OK\n"
     ]
    }
   ],
   "source": [
    "# === Validador express de documentación y artefactos (con normalización de DATASET.md) ===\n",
    "from pathlib import Path\n",
    "import re, hashlib, os\n",
    "\n",
    "# Usa las mismas rutas que venís usando:\n",
    "BASE = Path.cwd()\n",
    "DATA_DIR = BASE / \"data\"\n",
    "OUT_DIR  = BASE / \"dataset_docs\"\n",
    "CSV_PATH = DATA_DIR / \"ed2022_clean_min.csv\"  # ajustá si usás otro\n",
    "\n",
    "def sha256_of_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(p, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _normalize_dataset_md(ds_path: Path):\n",
    "    \"\"\"\n",
    "    Normaliza DATASET.md a las claves simples que el validador espera:\n",
    "      **Filas**: **N**\n",
    "      **Columnas**: **N**\n",
    "      **SHA256**: <HASH>\n",
    "    Convierte variantes como:\n",
    "      - **Instancias (filas)**: **N**\n",
    "      - **Características (columnas)**: **N**\n",
    "      - **SHA256 (CSV)**: `HASH`\n",
    "    \"\"\"\n",
    "    if not ds_path.exists():\n",
    "        return\n",
    "    txt = ds_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # Filas\n",
    "    # Ejemplos que convertimos a **Filas**: **N**\n",
    "    patterns_rows = [\n",
    "        r\"\\*\\*Instancias\\s*\\(filas\\)\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\",\n",
    "        r\"\\*\\*Filas\\s*\\(instancias\\)\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\",\n",
    "        r\"\\*\\*Filas\\s*\\(.*?\\)\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\",\n",
    "    ]\n",
    "    for pr in patterns_rows:\n",
    "        txt = re.sub(pr, r\"**Filas**: **\\1**\", txt)\n",
    "\n",
    "    # Columnas\n",
    "    # Ejemplos que convertimos a **Columnas**: **N**\n",
    "    patterns_cols = [\n",
    "        r\"\\*\\*Características\\s*\\(columnas\\)\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\",\n",
    "        r\"\\*\\*Caracter\\u00edsticas\\s*\\(columnas\\)\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\",  # por si viene con escape\n",
    "        r\"\\*\\*Columnas\\s*\\(características\\)\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\",\n",
    "        r\"\\*\\*Columnas\\s*\\(.*?\\)\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\",\n",
    "    ]\n",
    "    for pc in patterns_cols:\n",
    "        txt = re.sub(pc, r\"**Columnas**: **\\1**\", txt)\n",
    "\n",
    "    # SHA\n",
    "    # Variantes a **SHA256**: HASH (sin backticks)\n",
    "    patterns_sha = [\n",
    "        r\"\\*\\*SHA256\\s*\\(CSV\\)\\*\\*:\\s*`?([0-9a-f]{64})`?\",\n",
    "        r\"\\*\\*SHA256\\*\\*:\\s*`?([0-9a-f]{64})`?\",\n",
    "    ]\n",
    "    for ps in patterns_sha:\n",
    "        txt = re.sub(ps, r\"**SHA256**: \\1\", txt)\n",
    "\n",
    "    ds_path.write_text(txt, encoding=\"utf-8\")\n",
    "\n",
    "issues = []\n",
    "\n",
    "# 1) Existencia de archivos clave\n",
    "for p in [CSV_PATH, OUT_DIR/\"DATASET.md\", OUT_DIR/\"DATADICT.md\", OUT_DIR/\"SHA256SUMS.txt\"]:\n",
    "    if not p.exists():\n",
    "        issues.append(f\"Falta archivo: {p}\")\n",
    "\n",
    "if not issues:\n",
    "    # 1.b) Normalizar DATASET.md antes de validar su contenido\n",
    "    _normalize_dataset_md(OUT_DIR / \"DATASET.md\")\n",
    "\n",
    "    # 2) SHA real vs SHA256SUMS.txt\n",
    "    real_sha = sha256_of_file(CSV_PATH)\n",
    "    sums_txt = (OUT_DIR/\"SHA256SUMS.txt\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    m_sum = re.search(r\"^([0-9a-f]{64})\\s+(.+)$\", sums_txt.strip())\n",
    "    if not m_sum:\n",
    "        issues.append(\"Formato inválido en SHA256SUMS.txt (debe ser: '<SHA256>  <archivo>').\")\n",
    "    else:\n",
    "        sha_sums, file_sums = m_sum.group(1), m_sum.group(2)\n",
    "        if sha_sums != real_sha:\n",
    "            issues.append(f\"SHA no coincide con CSV. SHA real={real_sha} / SHA en archivo={sha_sums}\")\n",
    "        if Path(file_sums).name != CSV_PATH.name:\n",
    "            issues.append(f\"Nombre en SHA256SUMS.txt ({file_sums}) no coincide con CSV ({CSV_PATH.name}).\")\n",
    "\n",
    "    # 3) DATASET.md: filas, columnas, SHA (formato simple)\n",
    "    ds_txt = (OUT_DIR/\"DATASET.md\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    m_rows = re.search(r\"\\*\\*Filas\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\", ds_txt)\n",
    "    m_cols = re.search(r\"\\*\\*Columnas\\*\\*:\\s*\\*\\*(\\d+)\\*\\*\", ds_txt)\n",
    "    m_sha  = re.search(r\"\\*\\*SHA256\\*\\*:\\s*([0-9a-f]{64})\", ds_txt)\n",
    "\n",
    "    if not m_rows or not m_cols or not m_sha:\n",
    "        issues.append(\"DATASET.md no tiene alguno de: filas/columnas/SHA en el formato esperado.\")\n",
    "    else:\n",
    "        ds_rows = int(m_rows.group(1))\n",
    "        ds_cols = int(m_cols.group(1))\n",
    "        ds_sha  = m_sha.group(1)\n",
    "\n",
    "        if ds_sha != real_sha:\n",
    "            issues.append(f\"DATASET.md: SHA ({ds_sha}) no coincide con SHA real ({real_sha}).\")\n",
    "\n",
    "        # 5) DATADICT.md: filas/columnas\n",
    "        dd_txt = (OUT_DIR/\"DATADICT.md\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        m_dd_rows = re.search(r\"Filas\\:\\s\\*\\*(\\d+)\\*\\*\", dd_txt)\n",
    "        m_dd_cols = re.search(r\"Columnas\\:\\s\\*\\*(\\d+)\\*\\*\", dd_txt)\n",
    "        if m_dd_rows and m_dd_cols:\n",
    "            dd_rows = int(m_dd_rows.group(1))\n",
    "            dd_cols = int(m_dd_cols.group(1))\n",
    "            if dd_rows != ds_rows:\n",
    "                issues.append(f\"DATADICT.md vs DATASET.md: Filas {dd_rows} != {ds_rows}.\")\n",
    "            if dd_cols != ds_cols:\n",
    "                issues.append(f\"DATADICT.md vs DATASET.md: Columnas {dd_cols} != {ds_cols}.\")\n",
    "        else:\n",
    "            issues.append(\"DATADICT.md no expone 'Filas' y 'Columnas' en el encabezado.\")\n",
    "\n",
    "        # 6) Partes del diccionario (si las mencionás)\n",
    "        parts_declared = re.findall(r\"`(DATADICT_part\\d+\\.md)`\", dd_txt)\n",
    "        for p in parts_declared:\n",
    "            if not (OUT_DIR/p).exists():\n",
    "                issues.append(f\"Se declara {p} en DATADICT.md pero el archivo no existe.\")\n",
    "\n",
    "# Resultado\n",
    "if issues:\n",
    "    print(\"❌ VALIDADOR: HAY PROBLEMAS\")\n",
    "    for i, msg in enumerate(issues, 1):\n",
    "        print(f\"{i}. {msg}\")\n",
    "else:\n",
    "    print(\"✅ VALIDADOR: TODO OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee8086",
   "metadata": {},
   "source": [
    "## 5 Vistas útiles: variables con más faltantes y alta cardinalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3230339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>missing_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>RX27V3C3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>RX25V3C2</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>RX11CAT4</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>RX19V3C3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>RX23CAT3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>RX12V3C4</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>RX14V3C3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>RX23V3C3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>RX24V3C3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>RX12V1C4</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>RX12V2C4</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>RX24CAT3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>RX24V2C3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>RX11V3C4</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>RX12CAT4</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>RX11V1C4</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>RX11V2C4</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>RX21CAT3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>RX21V1C3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>RX21V2C3</td>\n",
       "      <td>99.9875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     variable  missing_pct\n",
       "745  RX27V3C3      99.9875\n",
       "720  RX25V3C2      99.9875\n",
       "496  RX11CAT4      99.9875\n",
       "641  RX19V3C3      99.9875\n",
       "686  RX23CAT3      99.9875\n",
       "528  RX12V3C4      99.9875\n",
       "564  RX14V3C3      99.9875\n",
       "693  RX23V3C3      99.9875\n",
       "708  RX24V3C3      99.9875\n",
       "520  RX12V1C4      99.9875\n",
       "524  RX12V2C4      99.9875\n",
       "700  RX24CAT3      99.9875\n",
       "705  RX24V2C3      99.9875\n",
       "508  RX11V3C4      99.9875\n",
       "516  RX12CAT4      99.9875\n",
       "500  RX11V1C4      99.9875\n",
       "504  RX11V2C4      99.9875\n",
       "659  RX21CAT3      99.9875\n",
       "662  RX21V1C3      99.9875\n",
       "665  RX21V2C3      99.9875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>DIAG1</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>ARRTIME_ts</td>\n",
       "      <td>1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOV</td>\n",
       "      <td>1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>DIAG2</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>DIAG3</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>MED1</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>MED2</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>MED3</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>MED4</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>MED5</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>DIAG4</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>HDDIAG1</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>MED6</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RFV2</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>DRUGID1</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>DRUGID2</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>MED7</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RFV1</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RFV3</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>DRUGID3</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       variable  unique\n",
       "54        DIAG1    1549\n",
       "768  ARRTIME_ts    1437\n",
       "3           LOV    1435\n",
       "55        DIAG2    1308\n",
       "56        DIAG3     989\n",
       "150        MED1     905\n",
       "151        MED2     891\n",
       "152        MED3     827\n",
       "153        MED4     736\n",
       "154        MED5     674\n",
       "57        DIAG4     668\n",
       "245     HDDIAG1     597\n",
       "155        MED6     590\n",
       "36         RFV2     552\n",
       "302     DRUGID1     540\n",
       "322     DRUGID2     530\n",
       "156        MED7     529\n",
       "35         RFV1     517\n",
       "37         RFV3     505\n",
       "342     DRUGID3     502"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_missing = dict_df.sort_values(\"missing_pct\", ascending=False)[[\"variable\",\"missing_pct\"]].head(20)\n",
    "high_card = dict_df[dict_df[\"unique\"]>500][[\"variable\",\"unique\"]].sort_values(\"unique\", ascending=False).head(20)\n",
    "display(top_missing)\n",
    "display(high_card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f052c-466e-4dcf-b3dc-a6b7daeaa330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
